
[深度卷积神经网络演化历史及结构改进脉络-40页长文全面解读](https://zhuanlan.zhihu.com/p/36765623)

# 早期成果
第一个真正意义上的CNN，LeCun在1989年提出。权重共享（weight sharing）和特征图像（feature map）。

# 深度卷积神经网络

# AlexNet
- 新的激活函数ReLU。
- dropout

## ReLU作用：
$ ReLU(x)=max(0,x) $。
- 其导数函数为$sng$。ReLU和它的导数计算简单。导数值为1。
- 在一定程度上解决梯度消失问题，训练时收敛更快。
- 网络更稀疏，起到类似L1正则化作用，缓解过拟合。

# ZFNet
反卷积（转置卷积）进行卷积网络可视化

# GoogLeNet
增加网络的规模，包括深度和宽度，带来两个问题：
（1）参数增加之后容易过拟合。
（2）计算量增加

Inception机制：
- 用多种尺寸的卷积核
- 1x1卷积进行降维（降低通道数）。
- **全局平均池化**替代全连接层。
- Inception如同小网络，反复堆叠形成大网络。

# VGG
- 采用了小尺寸的卷积核来模拟大尺寸的卷积核。
- 相比Alex去掉了LRN层，作者发现深度卷积网络中LRN作用不明显。

# Residual Network
- 跨层连接(shortcut connections)拟合残差项的手段解决深层网络中难以训练的问题。
- 实验表明，增加网络导数，会提高网络的性能。但随着增加到一定层数后，训练误差和测试误差会增大。这和过拟合不一样，过拟合只是测试集上的误差大，这个问题称为退化。

残差网络机制的结论：
（1）残差网络不是一个单一的超深网络，而是多个网络指数级的隐式集成，由此引入了多样性的概念，它用来描述隐式集成的网络的数量。
（2）在预测时，残差网络的行为类似于集成学习；
（3）对训练时的梯度流向进行了分析，发现隐式集成大多由一些相对浅层的网络组成。因此，残差网络并不能解决梯度消失问题。

# Inception-V2
（1）加入BN层，减少Internal Covariate Shift（内部neuron的数据分布发生变化），使每一层的输出都规范化到一个N（0，1）的高斯。
（2）学习VGG使用2个3X3代替Inception中的5x5

# Inception-V3
- 卷积核分解Factorization。将nxn的卷积核 分解成1xn和nx1。这样做既可以减少参数，又使网络加深，增加了网络的非线性。
- 新的参数优化的方式——RMSProp。可缓解AdaGrad下降较快的问题。
- Label Smoothing的策略。在输出标签中添加噪声，对模型进行约束，降低模型过拟合程度的一种正则化方法。


# Xception
对Inception V3的改进。Depthwise Separable Convolution替换原来的卷积操作。
原来，在一组特征图上进行卷积需要三维的卷积核，即卷积核 同时学习空间上的相关性和通道间的相关性。
Xception在卷积层加入group的策略，将学习空间相关性和通道间相关性的任务分享，大幅降低了模型的理论计算量且损失较少的准确度。


# Inception-ResNet v1/v2
将Inception V3/v4与残差网络思想进行融合。


# NasNet
- 通过学习自动产生网络结构
- 采用ResNet和Inception等成熟的网络结构 减少了网络结构优化的搜索空间，大型网络直接由大量同构模块堆叠而成，提高学习效率。

# WRN(wide residual network)

# ResNetXt

# DenseNet
密集连接的卷积神经网络。在该网络中，任何两层之间都有直接的连接。
网络更窄，参数更少。

# MobileNet
Separable Convolution将传统的卷积用两步卷积代替：Deptwise convolution和Pointwise convolution。
MobileNet-v2增加了残差结构。同时在Depthwise convolution之前 添加了一层Pointwise convolution，优化了带宽的作用。



