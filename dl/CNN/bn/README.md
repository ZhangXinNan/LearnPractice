
[《Batch Normalization原理与实战》](https://zhuanlan.zhihu.com/p/34879333?group_id=960898628487925760)

# 一、理论板块

## 1 提出背景
### 1.1 炼丹的困扰
神经网络难训练的原因存在高度的关联性和耦合性。

### 1.2 Internal Covariate Shift
> 在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。

### 1.3 Internal Covariate Shift会带来什么问题？
* (1) 上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低
* (2) 网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度

### 1.4 减缓Internal Covariate Shift
* (1) 白化 Whitening
白化是对输入数据分布进行变换，进而达到以下两个目的：
使得输入特征分布具有相同的均值与方差。其中PCA白化保证了所有特征分布均值为0，方差为1；而ZCA白化则保证了所有特征分布均值为0，方差相同；
去除特征之间的相关性。

* (2) Batch Normalization提出
白化过程计算成本太高
白化过程由于改变了网络每一层的分布，因而改变了网络层中本身数据的表达能力



## 2 算法思想
### 2.1 思路
既然白化计算过程比较复杂，那我们就简化一点，比如我们可以尝试单独对每个特征进行normalizaiton就可以了，让每个特征都有均值为0，方差为1的分布就OK。

另一个问题，既然白化操作减弱了网络中每一层输入数据表达能力，那我就再加个线性变换操作，让这些数据再能够尽可能恢复本身的表达能力就好了。

### 2.2 算法
Batch Normalization也就在mini-batch的基础上进行计算。
#### 2.2.1 参数定义
#### 2.2.2 算法步骤
#### 2.2.3 公式


### 2.3 参数定义


## 3 测试阶段如何使用BN


## 4 BN的优势 
1）BN使得网络中每层输入数据的分布相对稳定，加速模型学习速
2）BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定
3）BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题
4）BN具有一定的正则化效果


# 二、 实战板块


# 三、 总结

至此，关于Batch Normalization的理论与实战部分就介绍道这里。总的来说，BN通过将每一层网络的输入进行normalization，保证输入分布的均值与方差固定在一定范围内，减少了网络中的Internal Covariate Shift问题，并在一定程度上缓解了梯度消失，加速了模型收敛；并且BN使得网络对参数、激活函数更加具有鲁棒性，降低了神经网络模型训练和调参的复杂度；最后BN训练过程中由于使用mini-batch的mean/variance作为总体样本统计量估计，引入了随机噪声，在一定程度上对模型起到了正则化的效果。