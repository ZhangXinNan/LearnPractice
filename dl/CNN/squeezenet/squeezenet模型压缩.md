

从网络结构来看，SqueezeNet也算是设计精良了，但是最终性能还是要实验说话。论文作者将SqueezeNet与AlexNet在ImageNet上做了对比，值得注意的是，不仅对比了基础模型之间的差异，还对比了模型压缩的性能，其中模型压缩主要采用的技术有SVD，网络剪枝（network pruning）和量化（quantization）等。具体的对比结果如表2所示。

首先看一下基准模型的性能对比，SqueezeNet的Top-1优于AlexNet，Top-5性能一样，但是最重要的模型大小降低了50倍，从240MB->4.8MB，这个提升是非常有价值的，因为这个大小意味着有可能部署在移动端。

作者并没有止于此，而是继续进行了模型压缩。

1. 其中SVD就是奇异值分解，
2. 而所谓的网络剪枝就是在weight中设置一个阈值，低于这个阈值就设为0，从而将weight变成系数矩阵，可以采用比较高效的稀疏存储方式，进而降低模型大小。值得一提的Deep Compression技术，这个也是Han等提出的深度模型压缩技术，其包括网络剪枝，权重共享以及Huffman编码技术。这里简单说一下权重共享，其实就是对一个weight进行聚类，比如采用k-means分为256类，那么对这个weight只需要存储256个值就可以了，然后可以采用8 bit存储类别索引，其中用到了codebook来实现。关于Deep Compression详细技术可以参加文献[2]。从表2中可以看到采用6 bit的压缩，SqueezeNet模型大小降到了0.47MB，这已经降低了510倍，而性能还保持不变。为了实现硬件加速，Han等还设计了特定的硬件来高效实现这种压缩后的模型，具体参加文献[3]。
3. 顺便说过题外话就是模型压缩还可以采用量化（quantization），说白了就是对参数降低位数，比如从float32变成int8，这样是有道理，因为训练时采用高位浮点是为了梯度计算，而真正做inference时也许并不需要这么高位的浮点，TensorFlow中是提供了量化工具的，采用更低位的存储不仅降低模型大小，还可以结合特定硬件做inference加速。