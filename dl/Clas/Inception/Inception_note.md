Inception 学习笔记

# 0 前言
AlexNet 是 LeNet的扩展，应用了ReLU/Dropout等技巧。
VGG 泛化性能好，常用于图像特征的抽取、目标检测候选框生成等。参数量太大。
GoogLeNet 使用了Inception模块。它的目的是设计一种优良局部拓扑结构的网络，即对输入图像并行地执行多个卷积运算或者池化操作，即对输入图像并行地执行多个卷积或者池化操作，并将所有输出结果拼接为一个非常深的feature map。因为1x1，3x3，5x5等将不同的卷积与池化操作获得输入图像的不同信息，并行处理这些运算并结合能获得更好的图像表征。

# 1 Inception V1
一张图像具有总体特征和细节特征。
在每一尺度的feature map尽可能多做不同scale的分析，把想得到的不同来源的信息习可能得到。
问题：
* 目标大小差异大
* 位置差异大
* 网络太深了会过拟合
* 大的卷积层耗资源

解决方案：
* 使网络变宽，9个Inception模块，最后一个Inception模块使用全局平均池化。
* **使用3个不同大小的滤波器（1x1,3x3,5x5）对输入执行卷积操作。既增加了网络的width，又增加了网络对尺度的适应性。**
* 为降低算力，在3x3,5x5之前增加1x1卷积，限制输入通道数量。
* 避免梯度消失，增加两个辅助分类器。

# 2 Inception V2
问题：
* 减少特征表征瓶颈。减少维度会损失信息
* 因子分解更优秀，计算更高效

解决方案：
* 加入了BN层，减少了Internal Covariate Shift（内部neuro的数据分布变化），每一层输出都规范化到一个N(0,1)的高斯。
* 5x5 分解为两个3x3，降低参数数量，加速了计算。
* nxn的卷积核分解为1xn和nx1两个卷积
* 构建了三种不同类型的Inception模块（A/B/C）。

# 3 Inception V3
问题：
* 作者认为辅助分类器的作用是正则化

解决方法：
* RMSPorp优化器
* Factorized 7x7卷积
* 辅助分类器使用BatchNorm
* 标签平滑。添加到损失公式的一种正则，阻止 网络对某一类别过分自信，即阻止过拟合。


# 4 Inception V4
问题：
* 使模块更加一致
解决方法：
* 三个主要的Inception模块，称为A/B/C
* 引入了专用的缩减块，用于改变网络的宽度和调试。

# 5 Inception-ResNet V1和V2
问题：
* 引入残差连接，它Inception模块的卷积运算输出添加到输入上。

解决方案：
* 为了使残差加运算可行，卷积之后的输入和输出必须有相同的维度。因此，我们在初始卷积之后使用 1x1 卷积来匹配深度（深度在卷积之后会增加）。
* 主要 inception 模块的池化运算由残差连接替代。然而，你仍然可以在缩减块中找到这些运算。缩减块 A 和 Inception v4 中的缩减块相同。
* 如果卷积核的数量超过 1000，则网络架构更深层的残差单元将导致网络崩溃。因此，为了增加稳定性，作者通过 0.1 到 0.3 的比例缩放残差激活值。

# 参考
[从Inception v1到Inception-ResNet，一文概览Inception家族的「奋斗史」](https://zhuanlan.zhihu.com/p/37505777)
《深度学习轻松学习》冯超
《深度学习与计算机视觉-算法原理、框架应用与代码实现》叶韵
