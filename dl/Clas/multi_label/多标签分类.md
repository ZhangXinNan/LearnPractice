


# sigmoid函数形式
$$
S(x) = \frac 1 {1 + e^{-x}}
$$
常用于二元分类（binary classification）问题。


# softmax函数形式：
$$
S(x_j) = \frac {e^{x_j}} {\sum_{k=1}^{K}e^{x_k}}
$$

多元分类（multiclass classification），max函数只输出最大的那个值，而softmax则确保较小的值也有较小的概率，不会被直接舍弃掉，是一个比较soft的max。softmax输出值之和为1，sigmoid则不一定。

在只有两个类的情况下，softmax形式等价于simgoid形式。


# 问题整理
分类问题名称 | 输出层使用激活函数 | 对应的损失函数
--|--|--
二分类      |   sigmoid     |   二分类交叉熵损失函数    binary_crossentropy
多分类      |   softmax     |   多类别交叉熵损失函数    categorical_crossentropy
多标签分类  |   sigmoid     |   二分类交叉熵损失函数    binary_crossentropy


# pytorch 实现

## BCELoss

$$
l_n = -w_n [y_n \cdot \log{x_n} + (1 - y_n)\log{(1 - x_n)}]
$$



## BCEWithLogitsLoss





# 参考资料

* [pytorch 多标签分类以及BCEloss](https://blog.csdn.net/ganxiwu9686/article/details/103080695)

