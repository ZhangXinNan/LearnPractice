{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.1 Tensor\n",
    "Tensor和numpy的ndarray类似，但 Tensor可以用GPU加速。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建5x3矩阵\n",
    "x = t.Tensor(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2158, 0.8559, 0.0100],\n",
       "        [0.0109, 0.0666, 0.6713],\n",
       "        [0.3080, 0.1262, 0.7858],\n",
       "        [0.5803, 0.4042, 0.9556],\n",
       "        [0.3371, 0.9858, 0.8290]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用0,1均匀分布随机初始化二维数组\n",
    "x = t.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# 查看x的形状\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1461, 0.3779, 0.1834],\n",
      "        [0.9683, 0.7272, 0.6269],\n",
      "        [0.8087, 0.9414, 0.8622],\n",
      "        [0.6123, 0.4662, 0.2814],\n",
      "        [0.4456, 0.5415, 0.8260]])\n",
      "tensor([[0.3619, 1.2338, 0.1933],\n",
      "        [0.9792, 0.7938, 1.2983],\n",
      "        [1.1167, 1.0676, 1.6480],\n",
      "        [1.1926, 0.8704, 1.2370],\n",
      "        [0.7827, 1.5273, 1.6550]])\n",
      "tensor([[0.3619, 1.2338, 0.1933],\n",
      "        [0.9792, 0.7938, 1.2983],\n",
      "        [1.1167, 1.0676, 1.6480],\n",
      "        [1.1926, 0.8704, 1.2370],\n",
      "        [0.7827, 1.5273, 1.6550]])\n",
      "tensor([[0.3619, 1.2338, 0.1933],\n",
      "        [0.9792, 0.7938, 1.2983],\n",
      "        [1.1167, 1.0676, 1.6480],\n",
      "        [1.1926, 0.8704, 1.2370],\n",
      "        [0.7827, 1.5273, 1.6550]])\n"
     ]
    }
   ],
   "source": [
    "y = t.rand(5, 3)\n",
    "print(y)\n",
    "# 3种加法\n",
    "print(x + y)\n",
    "print(t.add(x, y))\n",
    "result = t.Tensor(5, 3)\n",
    "t.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2158, 0.8559, 0.0100],\n",
      "        [0.0109, 0.0666, 0.6713],\n",
      "        [0.3080, 0.1262, 0.7858],\n",
      "        [0.5803, 0.4042, 0.9556],\n",
      "        [0.3371, 0.9858, 0.8290]])\n",
      "tensor([[0.1461, 0.3779, 0.1834],\n",
      "        [0.9683, 0.7272, 0.6269],\n",
      "        [0.8087, 0.9414, 0.8622],\n",
      "        [0.6123, 0.4662, 0.2814],\n",
      "        [0.4456, 0.5415, 0.8260]])\n",
      "tensor([[0.1461, 0.3779, 0.1834],\n",
      "        [0.9683, 0.7272, 0.6269],\n",
      "        [0.8087, 0.9414, 0.8622],\n",
      "        [0.6123, 0.4662, 0.2814],\n",
      "        [0.4456, 0.5415, 0.8260]])\n",
      "tensor([[0.3619, 1.2338, 0.1933],\n",
      "        [0.9792, 0.7938, 1.2983],\n",
      "        [1.1167, 1.0676, 1.6480],\n",
      "        [1.1926, 0.8704, 1.2370],\n",
      "        [0.7827, 1.5273, 1.6550]])\n"
     ]
    }
   ],
   "source": [
    "# 函数名后带下划线的函数会修改Tensor本身\n",
    "print(x)\n",
    "print(y)\n",
    "y.add(x)\n",
    "print(y)\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Tensor不支持的操作，可以先转为numpy处理，之后 再转回Tensor。\n",
    "# Tensor和numpy对象共享内存，所以转换非常快。\n",
    "a = t.ones(5)\n",
    "b = a.numpy()\n",
    "c = t.from_numpy(b)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor可通过.cuda方法转为GPU的Tensor，从而 用GPU加速。 \n",
    "# t.cuda.is_available()判断cuda是否可用。\n",
    "if t.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Autograd：自动微分\n",
    "autograd.Variable是Autograd的核心类，它封装了Tensor，包括data/grad/grad_fn（指向一个function）。用.backware实现反向传播，自动计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "x = Variable(t.ones(2,2), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor(4., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "y = x.sum()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
