# 九、神经网络的学习（Neural Networks:Learning）
## 9.1 代价函数（Cost Function） 
【参考视频: 9 - 1 - Cost Function (7 min).mkv】

训练样本：${(x^1,y^1),(x^2,y^2),...,(x^n,y^n)}$
$L$ = total no. of layers in network
$s_l$ = no. of units (not counting bias unit) in layer $l$

**逻辑回归中的代价函数（Logistic regression）** :
$$J(\theta)=-\frac1m[\sum^m_{i=1}(y^ilog h_\theta(x^i)+(1-y^i)log(1-h_\theta(x^i))]+\frac{\lambda}{2m}\sum^n_{j=1}\theta^2_j$$

**Neural network**:
$$h_\Theta(x)\in\mathbb{R}^K$$
$$h_\Theta(x)_i=i^{th} output$$
$$J(\Theta)=-\frac1m[\sum^m_{i=1}\sum^K_{k=1}(y^i_klog (h_\Theta(x^i))_k+(1-y^i_k)log(1-h_\theta(x^i))_k]+\frac{\lambda}{2m}\sum^{L-1}_{l=1}\sum^{s_l}_{i=1}\sum^{s_l+1}_{j=1}(\Theta^l_ji)^2$$



##9.2 反向传播算法（Backpropagation Algorithm）
【参考视频: 9 - 2 - Backpropagation Algorithm (12 min).mkv】
**Gradient computation** :
$$min_{\Theta}J(\Theta)$$

**Need code to compute** :
$$J(\Theta)$$
$$\frac{\partial}{\partial \Theta^l_{ij}}J(\Theta)$$

**Given one training example** $$(x, y)$$
**前向传播算法（Froward propagation）** :
$a^1 = x$
$z^2 = \Theta^1a^1$
$a^2= g(z^2)$ (add $a^2_0$)
$z^3=\Theta^2a^2$
$a^3=g(z^3) $ (add $a^3_0$)
$z^4 = \Theta^3a^3$
$a^4=h_\Theta(x) = g(z^4)$
![这里写图片描述](http://img.blog.csdn.net/20160425172304461)

Gradient computation : Backpropagaton algorithm
Intuition : $\delta^l_j$ = "error " of node $j$ in layer $l$
For each output unit (layer L=4)
$\delta^4_j=a^4_j-y_j$
$\delta^3=(\Theta^3)^T\delta^4 .* g^{'}(z^3)$
$\delta^2=(\Theta^2)^T\delta^3 .* g^{'}(z^2)$


Training set $(x^1, y^1), ...,(x^m,y^m)$
Set $\Delta^l_{ij}=0$ (for all l , i, j)
For $i = 1$ to $m$
    Set $a^1 = x^i$
    Perform forward propagation to compute $a^l$ for l =2,3,...,L
    Using $y^i$, compute $\delta^L=a^L-y^i$
    Compute $\delta^{L-1}, \delta^{L-2},...,\delta^2$
    $\Delta^l_{ij} := \Delta^l_{ij} + a^l_j\delta^{l+1}_i$ 

即首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。

$D^l_{ij} := \frac1m\Delta^l_{ij}$ if $j = 0$
$D^l_{ij} := \frac1m\Delta^l_{ij} + a^l_j\delta^{l+1}_i$ otherwise

## 9.3 反向传播算法的直观理解
【参考视频: 9 - 3 - Backpropagation Intuition (13 min).mkv】

## 9.4 实现注意：展开参数（Implementation Note : Unrolling Parameters）
【参考视频: 9 - 4 - Implementation Note_ Unrolling Parameters (8 min).mkv】


##9.5 梯度检验 （Gradient Checking） 
【参考视频: 9 - 5 - Gradient Checking (12 min).mkv】
梯度检验
单侧拆分$\frac{\partial }{\partial \theta}J(\theta) = \frac{J(\theta+\varepsilon)-J(\theta)}{\varepsilon}$
双侧拆分$\frac{\partial }{\partial \theta}J(\theta) = \frac{J(\theta+\varepsilon)-J(\theta-\varepsilon)}{2\varepsilon}$

##9.6 随机初始化（Random Initialization）

##9.7 综合起来（Putting It together）
【参考视频: 9 - 7 - Putting It Together (14 min).mkv】


##9.8 自主驾驶 （Autonomous Driving）
【参考视频: 9 - 8 - Autonomous Driving (7 min).mkv】

