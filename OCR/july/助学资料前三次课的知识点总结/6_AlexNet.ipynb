{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [参考](https://blog.csdn.net/qq_24695385/article/details/80368618)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alexnet结构\n",
    "\n",
    "* alexNet为8层深度网络，其中5层卷积层和3层全连接层，不计LRN层和池化层。如下图所示：\n",
    "    * ![](https://img-blog.csdn.net/2018062420005650?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI2Nzk3MDc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "    * [LRN(局部响应归一化)](https://blog.csdn.net/hduxiejun/article/details/70570086)\n",
    "        * 典型的如: ReLU\n",
    "\n",
    "\n",
    "* 详解各层训练参数的计算：\n",
    "    * 前五层：卷积层\n",
    "        * ![](https://img-blog.csdn.net/20180624204850119?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI2Nzk3MDc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "    * 后三层：全连接层\n",
    "        * ![](https://img-blog.csdn.net/20180624204954232?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI2Nzk3MDc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "    * 整体计算图：\n",
    "        * ![](https://img-blog.csdn.net/2018062420511556?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI2Nzk3MDc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结构分析\n",
    "\n",
    "* AlexNet每层的超参数如下图所示，其中输入尺寸为227*227，第一个卷积使用较大的核尺寸11*11，步长为4，有96个卷积核；紧接着一层LRN层；然后是最大池化层，核为3*3，步长为2。这之后的卷积层的核尺寸都比较小，5*5或3*3，并且步长为1，即扫描全图所有像素；而最大池化层依然为3*3，步长为2.\n",
    "\n",
    "* 我们可以发现，前几个卷积层的计算量很大，但参数量很小，只占Alexnet总参数的很小一部分。这就是卷积层的优点！通过较小的参数量来提取有效的特征。\n",
    "\n",
    "* ![](https://img-blog.csdn.net/2018062420064020?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI2Nzk3MDc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet的新技术点\n",
    "\n",
    "* AlexNet的新技术点（即大牛论文的contribution），如下：\n",
    "    * ReLU作为激活函数。\n",
    "        * ReLU为非饱和函数，论文中验证其效果在较深的网络超过了SIgmoid，成功解决了SIgmoid在网络较深时的梯度弥散问题。\n",
    "    * Dropout避免模型过拟合\n",
    "        * 在训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。在alexnet的最后几个全连接层中使用了Dropout。\n",
    "\n",
    "    * 重叠的最大池化\n",
    "        * 之前的CNN中普遍使用平均池化，而Alexnet全部使用最大池化，避免平均池化的模糊化效果。并且，池化的步长小于核尺寸，这样使得池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。\n",
    "\n",
    "    * 提出LRN层\n",
    "        * 提出LRN层，对局部神经元的活动创建竞争机制，使得响应较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。\n",
    "\n",
    "    * GPU加速\n",
    "\n",
    "    * 数据增强\n",
    "        * 随机从256*256的原始图像中截取224*224大小的区域（以及水平翻转的镜像），相当于增强了（256-224）*（256-224）*2=2048倍的数据量。使用了数据增强后，减轻过拟合，提升泛化能力。避免因为原始数据量的大小使得参数众多的CNN陷入过拟合中。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
