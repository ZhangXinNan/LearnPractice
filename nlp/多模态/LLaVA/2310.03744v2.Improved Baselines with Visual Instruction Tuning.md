
Improved Baselines with Visual Instruction Tuning

# Abstract

Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in∼1 day on a single 8-A100 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available.

大型多模态模型（LMM）最近在视觉指令微调方面取得了令人鼓舞的进展。在本文中，我们首次在受控环境下，基于 LLaVA 框架对 LMM 的设计选择进行了系统研究。我们发现，LLaVA 中的全连接视觉-语言连接器功能强大且数据效率极高。通过对 LLaVA 进行简单的修改，即使用 CLIP-ViT-L-336px 并添加 MLP 投影，以及添加面向学术任务的 VQA 数据和响应格式提示，我们构建了更强的基线模型，并在 11 个基准测试中取得了最先进的性能。我们最终的 13B 模型仅使用了 120 万个公开可用的数据，并在单个 8-A100 节点上仅用约 1 天时间就完成了完整训练。此外，我们还对 LMM 中的一些开放性问题进行了初步探索，包括扩展到更高分辨率的输入、组合能力和模型幻觉等。我们希望这项工作能够使最先进的 LMM 研究更加易于理解和应用。代码和模型将公开发布。

# 1. Introduction
Large multimodal models (LMMs) have become increasingly popular in the research community, as they are the key building blocks towards general-purpose assistants [2, 30, 43]. Recent studies on LMMs are converging on a central concept known as visual instruction tuning [36]. The results are promising, e.g. LLaVA [36] and MiniGPT-4 [62] demonstrate impressive results on natural instruction-following and visual reasoning capabilities. To better understand the capability of LMMs, multiple benchmarks [17, 27, 34, 37, 55] have been proposed. Recent works further demonstrate improved performance by scaling up the pretraining data [3, 14, 54], instruction-following data [14, 18, 29, 58], visual encoders [3], or language models [39], respectively. The LLaVA architecture is also leveraged in different downstream tasks and domains, including region-level [8, 56] and pixel-level [26, 50] understanding, biomedical assistants [31], image generation [5], adversarial studies [6, 59].

大型多模态模型（LMMs）在研究领域越来越受欢迎，因为它们是构建通用助手的关键组成部分 [2, 30, 43]。近期对 LMMs 的研究正趋向于一个名为视觉指令微调的核心概念 [36]。研究结果令人鼓舞，例如 LLaVA [36] 和 MiniGPT-4 [62] 在自然指令遵循和视觉推理能力方面展现出令人印象深刻的性能。为了更好地理解 LMMs 的能力，研究人员提出了多种基准测试 [17, 27, 34, 37, 55]。最近的研究表明，通过扩展预训练数据 [3, 14, 54]、指令遵循数据 [14, 18, 29, 58]、视觉编码器 [3] 或语言模型 [39]，可以进一步提升性能。LLaVA 架构也被应用于不同的下游任务和领域，包括区域级 [8, 56] 和像素级 [26, 50] 理解、生物医学助手 [31]、图像生成 [5] 和对抗性研究 [6, 59]。



