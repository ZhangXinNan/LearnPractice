

RNN算法只能从左向右或从右向左依次计算，带来两个问题：
（1） 时间t的计算依赖t-1时刻的计算结果，限制了模型的并行能力。
（2） 顺序计算的过程中信息会丢失，尽管 LSTM 等门控机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象，LSTM 依旧无能为力。

Transformer的提出解决了上面两个问题： 
（1）首先它使用了Attention机制，将序列中的任意两个位置之间的距离缩小为一个常量（即 其他时刻对于当前时刻的影响通过一步就可以计算得到）；  
（2）其次它不是类似 RNN 的顺序结构，因此具有更好的并行性，符合现有的GPU框架。


# 参考资料
[有没有比较详细通俗易懂的Transformer教程？](https://www.zhihu.com/question/485876732/answer/2346024378)