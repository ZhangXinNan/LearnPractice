#八、神经网络：表述（Neural Networks:Representation）

##8.1 非线性假设（Non-linear Hypotheses）
【参考视频: 8 - 1 - Non-linear Hypotheses (10 min).mkv】

##8.2 神经元和大脑（Neurons and the Brain）
So, it's pretty amazing to what extent is as if you can plug in almost any sensor to the brain and brain's learning algorithm will just figure out how to learn from that data and deal with that data.

## 8.3 模型表示1（Model Representation 1）
neuron in our brain 
dendrite(input)(树突)-->cell body-->axon(output)(轴突)-->axon terminal
激活单元 activation unit
参数/权重 weight
输入单元 input units
输入层 input layer
输出层 output layer
隐藏层 hidden layers
偏差单位 bias unit
![神经网络示例](http://img.blog.csdn.net/20160420145716455)

* $a^j_i$ 第j层的第i个激活单元（ activation of uint $i$ in layer $j$）
* $\theta^j$ 第j层遇到到第j+1层的权重的矩阵，以第j+1层的激活单元数量为行数，以第j层的激活单元数加1为列数。（matrix weights controlling function mapping from layer $j$ to layer $j+1$）
* sigmoid(logistic) activation function : $g(z)=\frac1{1+e^{-z}}$
$a^2_1=g(\theta^1_{10}x_0 + \theta^1_{11}x_1+\theta^1_{12}x_2+\theta^1_{13}x_3)$
$a^2_2=g(\theta^1_{20}x_0 + \theta^1_{21}x_1+\theta^1_{22}x_2+\theta^1_{23}x_3)$
$a^2_3=g(\theta^1_{30}x_0 + \theta^1_{31}x_1+\theta^1_{32}x_2+\theta^1_{33}x_3)$
$h_\theta(x)=a^3_1=g(\theta^2_{10}a^2_0+\theta^2_{11}a^2_1+\theta^2_{12}a^2_2+\theta^2_{13}a^2_3)$

每一个a由上一层所有的x和每一个x所对应的参数决定的。
把这样从左到右的算法称为**前向传播算法（forward propagation）**。
把x,a，分别用矩阵来表示：
$$X = \begin{bmatrix}
x_0\\
x_1\\
x_2\\
x_3
\end{bmatrix}$$
$$\theta = \begin{bmatrix}
\theta_{10} &\theta_{11}  &\theta_{12}  &\theta_{13} \\ 
\theta_{20} &\theta_{21}  &\theta_{22}  &\theta_{23}  \\ 
\theta_{30} &\theta_{31}  &\theta_{32}  &\theta_{33}  
\end{bmatrix}$$
$$a = \begin{bmatrix}
a_1\\
a_2\\
a_3
\end{bmatrix}$$
我们可以得到：
$$a = \Theta X$$

## 8.4 模型表示2（Model Representation 2）
$x = 
\begin{bmatrix}
x_0\\
x_1\\
x_2\\
x_3
\end{bmatrix}$

$\Theta^1 =
 \begin{bmatrix}
\theta^1_{10} &\theta^1_{11}  &\theta^1_{12}  &\theta^1_{13} \\ 
\theta^1_{20} &\theta^1_{21}  &\theta^1_{22}  &\theta^1_{23}  \\ 
\theta^1_{30} &\theta^1_{31}  &\theta^1_{32}  &\theta^1_{33}  
\end{bmatrix}$

$z^2=
\begin{bmatrix}
z^2_1 \\
z^2_2 \\
z^2_3
\end{bmatrix}$

$a^2=
\begin{bmatrix}
a^2_0 \\
a^2_1 \\
a^2_2 \\
a^2_3
\end{bmatrix}$

$\Theta^2 =
 \begin{bmatrix}
\theta^2_{10} &\theta^2_{11}  &\theta^2_{12}  &\theta^2_{13}
\end{bmatrix}$

$z^2 = \Theta^1 x$
$a^2 = g(z^2)$
$z^3=\Theta^2 a^2$
$h_\theta(x)=a^3=g(z^3)$

如果对整个训练集进行计算，需要将训练集特征矩阵进行转置，使得同一个实例的特征都在同一列里，即：
$z^2 = \Theta X^T$
$a^2 = g(z^2)$

## 8.5 特征和直观理解（Examples and Intuitions 1）
【参考视频: 8 - 5 - Examples and Intuitions I (7 min).mkv】

Non-linear classification example : XOR/XNOR
$x_1, x_2 \in \{0, 1\}$
$$y = x_1 AND  x_2$$
Example : And function
$\theta_0 = -30$
$\theta_1 = 20$
$\theta_2 = 20$ 
$$y = x_1 OR  x_2$$
Example: Or function
$\theta_0 = -10$
$\theta_1 = 20$
$\theta_2 = 20$ 

##8.6 样本和直观理解 II(Examples and Intuitions 2)
Example: XNOR function
$\theta^1_{1i}=[-30,  20,  20]$
$\theta^1_{2i}=[ 10, -20, -20]$
| x1 | x2 | a1 | a2 | $h_\theta(x)$ 
| ---| ---| ---| ---| ---
|0 |0  |0   |1     |1     
|0 |1  |0   |0     |0
|1 |0  |0   |0    |0
|1 |1  |1   |0     |1




##8.7 多分类 （Multiclass Classification）
【参考视频: 8 - 7 - Multiclass Classification (4 min).mkv】


