# 12 支持向量积
## 12.1 优化目标
在监督学习中，许多学习算法的性能都非常相似，重要的不是该选择何种算法，而是创建的大量数据在应用这些算法时表现情况依赖于你的水平。比如：你为学习算法所设计的特征量的选择，以及如何选择正则化参数。我们从优化目标开始。我们将从逻辑回归开始展示我们如何一点一点修改来得到本质上的支持向量积。

令$z = \theta^Tx$，逻辑回归的假设函数：
$$h_{\theta}(x)=\frac1{1+e^{-z}}$$
![这里写图片描述](http://img.blog.csdn.net/20160501133945377)

逻辑回归的代价函数：
$$-y \log h_\theta(x) - (1-y) \log (1 - h_\theta(x))$$
![这里写图片描述](http://img.blog.csdn.net/20160501134248101)
Logistic regression:
$$min_\theta \frac 1 m \sum^m_{i=1}[y^i(-\log h_\theta(x^i)) + (1-y^i)(-\log(1-h_\theta(x^i)))] + \frac \lambda{2m}\sum^n_{j=1}\theta_j^2 $$

Support vector machine :
$$min_\theta  C\sum^m_{i=1}[y^i cost_1(\theta^Tx^i) + (1-y^i)cost_0(\theta^Tx^i)] + \frac 1{2}\sum^n_{j=1}\theta_j^2 $$

![这里写图片描述](http://img.blog.csdn.net/20160509144517374)
## 12.2 大边界的直观理解
【参考视频: 12 - 2 - Large Margin Intuition (11 min).mkv】


## 12.3 数学背后的大边界分类
【参考视频: 12 - 2 - Large Margin Intuition (11 min).mkv】
$$u=\begin{bmatrix}
u_1\\ 
u_2
\end{bmatrix}$$

$$v=\begin{bmatrix}
v_1\\ 
v_2
\end{bmatrix}$$

向量u和v之间的内积：$u^Tv$。
u的范数：$\left \| u \right \|=\sqrt{u_1^2 + u_2^2}$，这是向量u的长度。
p是v投影到向量u上的长度，$$u^Tv = p \cdot \left \| u\right \|$$

注意：p是有符号的，有可能是正值也可能是负值。


## 12.4 核函数1

## 12.5 核函数2

## 12.6 使用SVM
从逻辑回归模型，我们得到了svm，在两者之间如何选择？
下面是普遍使用的原则：
n为特征数，m为训练样本数。
（1）n >> m，即训练数据量不够支持我们训练一个复杂的非线性模型，则选用逻辑回归模型或者不带核函数的支持向量积。
（2）n较小，m一般，例如 1 < n < 1000，10 < m < 1000，使用高斯核函数的SVM。
（3）n较小，m较大，例如1<n<1000，m>50000，则使用SVM非常慢，解决方案是创造、增加更多特征，然后使用逻辑回归或不带高斯核函数的支持向量积。
 