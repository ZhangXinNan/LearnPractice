
Putting it together

# Decesion Tree Learning
1. Start with all examples at the root node
2. Calculate information gain for all possible features, and pick the one with the highest information gain
3. Split dataset according to selected feature, and create left and right branches of the tree
4. Keep repeating splitting process until stopping criteria is met.
   1. When a node is 100% one class
   2. When splitting a node will result in the tree exceeding a maximum depth
   3. Information gain from additional splits is less than threshold
   4. When number of examples in a node is below a threshold




